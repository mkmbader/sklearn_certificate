{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Supervised learning - linear models](https://scikit-learn.org/stable/modules/linear_model.html#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Unsupervised learning: extract from data X a structure that can be generalized\n",
    "\n",
    "#### Supervised learning - Linear models\n",
    "* Target is linear combination of feature: coef_, intercept_, .fit()\n",
    "* Linear regression: minimize the residual sum between observed and predicted target\n",
    "* Important that features are independent of each other\n",
    "* Evaluate with mean square error\n",
    "* Coefficients can be forced to be non-negative: positive parameter\n",
    "* Does not have a classifier with it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "reg.fit(X, y)\n",
    "print('coefficients', reg.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Ridge regression and classification](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)\n",
    "\n",
    "* Penalty **equal to the square of the magnitude of coefficients** on the coefficients of correlated predictors (**shrinks** them)\n",
    "* Tries to handle multicolinearity\n",
    "* good when you want to keep most featuers, but want to remove the effect of correlation\n",
    "* When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero.\n",
    "* comes with a classifier (`RidgeClassifier`) -> Least Squares Support Vector Machines with a linear kernel\n",
    "* Cross-validation: `RidgeCV` and `RidgeClassifierCV`: work in the same way as `GridSearchCV` except that it defaults to efficient Leave-One-Out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients  [0.44444444 0.44444444]\n",
      "intercept  0.11111111111111116\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit(X, y)\n",
    "print('coefficients ', reg.coef_)\n",
    "print('intercept ',  reg.intercept_)\n",
    "\n",
    "regCV = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below there are multiple alphas, for each the MSE is calculated and the optimal alpha is chosen based on the best negative mean squared error (default) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas  1e-06\n",
      "coefficients  [0.49999988 0.49999988]\n",
      "intercept  2.491287887096405e-07\n"
     ]
    }
   ],
   "source": [
    "regCV = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "regCV.fit(X, y)\n",
    "\n",
    "\n",
    "print('alpha ', regCV.alpha_)\n",
    "print('coefficients ', regCV.coef_)\n",
    "print('intercept ',  regCV.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Lasso regression](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "* Penalty **equal to the absolute value of the coefficients** on the coefficients\n",
    "* Can set coefficients to exactly zero\n",
    "* good when there are many featuers, and you want to reduce them\n",
    "* Cross-validation: there is \n",
    "    * `LassoCV`: \n",
    "        * coordinate descent -> optimizes one coefficient at a time \n",
    "        * simpler and straight forward implementation\n",
    "        * advised for data with many correlated features\n",
    "    * `LassoLarsCV`: \n",
    "        * optimizes all coefficients at the same time by moving the coefficients in the direction of the most correlated features and updates all coefficients simultaneously\n",
    "        * efficient in contexts where the number of features is significantly greater than the number of samples\n",
    "        * often faster than LassoCV, more relevant parameters of alpha\n",
    "\n",
    "\n",
    "Some notes on [being cautious with coefficients](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py):\n",
    "* coefficients vary significantly when changing the input dataset their robustness is not guaranteed (train/test) - two or more features could be correlated\n",
    "\n",
    "\n",
    "Other models:\n",
    "* `LassoLarsIC` uses Akaike Information Criterion (AIC) and Mayes Information Criterion (BIC): the lower the better the model fits based on the # parameters\n",
    "* `alpha`  and regularization parameter `C` of SVM are connected with `alpha = 1/C`\n",
    "* `MulitTaskLasso`: handles multiple regression problems jointly. Constraint: selected features are the same for all the regression problems\n",
    "* `ElasticNetCV` & `MultiTaskElasticNet`: Ridge + Lasso regression\n",
    "* `OrthogonalMatchinPursuit`: a linear model that allows to fix the number of desired non-zero coefficients\n",
    "* `Bayesian(Ridge)Regression`: introduce uninformative priors to the Ridge regression/classification model. Then `alpha` is like a random variable determined by the data\n",
    "* `ARDRegression`: Automatic Relevance Determination, similar to Bayesin Ridge regression, but leads to sparser coefficients (different prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "* implemented as a linear model for classification\n",
    "* implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional or Elastic-Net regularization\n",
    "* Regularization is applied \n",
    "* numerical output is the predicted probability -> classifier when setting threshold\n",
    "* `.predict` -> class lables, `.predict_proba` -> predict probabilities\n",
    "* binary case can be extended to K classes leading to the multinomial logistic regression\n",
    "\n",
    "\n",
    "#### [Solvers](https://scikit-learn.org/stable/modules/linear_model.html#differences-between-solvers)\n",
    "* Characteristics: performance, speed, and suitability for different types of data\n",
    "* `lbfgs` (default): small to medium-sized datasets; works well with multi-class problems and provides good convergence properties.\n",
    "* `liblinear`: designed for large,sparse data and linear classification\n",
    "* `newton-cg`: Suitable for problems where the number of features is much smaller than the number of samples \n",
    "* `newton-cholesky`: particularly well-suited for datasets where the features are dense (i.e., the majority of feature values are non-zero)\n",
    "* `sag`: large datasets\n",
    "* `saga`: Opens up more flexibility with regularization options while maintaining the efficiency of SAG.\n",
    "\n",
    "\n",
    "#### [Generalized Linear Models](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models)\n",
    "* allow to define your own probability density function, based on target distribution eg \n",
    "    * Poisson: (if target is relative counts or frequency, thus non-negative)\n",
    "    * Gamma: If the target values are positive valued and skewed\n",
    "    * inverse gaussian: If the target values seem to be heavier tailed than a Gamma distribution\n",
    "    * Bernoulli: If the target values are probabilities\n",
    "* `TweedieRegressor` implement the GLM using the `power` parameter\n",
    "\n",
    "\n",
    "### Algorithms for large scale learning (> samples & features)\n",
    "#### [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd)\n",
    "* `SGDClassifier`, `SGDRegressor`\n",
    "* Fits different know model types depending on the loss function\n",
    "    * `loss = 'log'` -> logistic regression\n",
    "    * `loss = \"hinge\"` -> linear SVM\n",
    "\n",
    "#### [Perceptron](https://scikit-learn.org/stable/modules/linear_model.html#perceptron)\n",
    "* No leanring rate, no regularization, only learning on previous mistakes\n",
    "\n",
    "#### [Passive Agressive Algorithms](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms)\n",
    "* No learning rate, but require regularization parameter\n",
    "\n",
    "### Algorithms for dealing with outliers and modelling errors\n",
    "* [Senarios](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors): Outliers in X & y, number of outliers, amplitude of outliers\n",
    "* Methods use a subset of the data to fit/complete the dataset\n",
    "* `HuberRegressor`: fast for << nsamples, does not ignore, but gives less weight to outliers (like ridge)\n",
    "* `RANSAC`: good for large outliers in y-direction, uses a subset of the data\n",
    "* `Theil Sen`: better with medium ouliers in X direction, but not good >> n features, uses a generalization of the median\n",
    "\n",
    "### Other models:\n",
    "* Predicting intervals - [Quantile regression](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression)\n",
    "* [Polynomial regression](https://scikit-learn.org/stable/modules/linear_model.html#quantile-regression) - extend the linear model with non-linear data -> `PolynomialFeatures`. Allows to fit wider range of data while mainaining speed of linear algos\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn_certificate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
