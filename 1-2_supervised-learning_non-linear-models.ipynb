{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Linear and Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/lda_qda.html#estimation-algorithms)\n",
    "\n",
    "* Classifiers with closed form solution\n",
    "* Simple: multiclass & no hyperparameters\n",
    "* Linear and quadratic decision bounds\n",
    "* `LinearDiscriminantAnalysis` can be used for dimensionality reduction and is available in the `.transform` method\n",
    "* `QDA` is eqiuvalent to `naive_bays.GaussianNB` if the inputs are conditionally independend in each class\n",
    "* Regularization/Shrinkage can be used to improve the generalization performance of the classifier\n",
    "    * set solver to `lsqr` or `eigen`\n",
    "    * set shrinkage to `auto`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "* Classification (`SVC`, `NuSVC`, `LinearSVC`), regression, outliers detection\n",
    "* Support vectors -> subset of training points within the margin (`.support_vectors_`). Cost function for building the model does not care about training points that lie on or beyond the margin.\n",
    "    * Margin: D(decision boudary, closest data points of each class), Goal -> maximize the margin (larger margin, better generalization)\n",
    "    * Deicion boundary: plane separating different classes\n",
    "* good where n features > n samples \n",
    "* They can handle non-linear decision bounds - so they are good for classifying complex data\n",
    "* Binary and Multi-class classification: one-versus-one for multi-class classification - binary classifier for every possible pair of classes\n",
    "* `SVC`, `NuSVC`: one-vs-one and then map to one-vs-rest with `decsiion_function` for per-class score\n",
    "* Scores are scaled with the Platt method (logit + cross-validation), but this has *limitations*\n",
    "    * Computationally expensive -> Kernel choice for optimization\n",
    "    * probability estimates can be incosistent with the scores (`predict` gives positive class even if `.predict_proba` < threshold)\n",
    "    * \"theoretical issues\"\n",
    "* has `class_weight` and `sample_weicth` method to deal with imbalance\n",
    "* Regularization is present\n",
    "* *Data has to be scaled!*\n",
    "* Outlier detection `OneClassSVM` -> unsupervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/sgd.html#stochastic-gradient-descent)\n",
    "* A way to fit a model, such as linear classifiers & regressors (eg linear SVM, logit)\n",
    "* Efficient (fast) in large scale (>10k) and spare matrix ML problems, like text classification or NLP\n",
    "* Allows code tuning (from scratch), but requires hyperparameters\n",
    "* **Requires features caling!**\n",
    "* Training data needs to be shuffled\n",
    "* Regularization: good for >10k samples, otherwiase use Ridge, Lasso, ElasticNet\n",
    "* `early_stopping` -> True: train/test split, False: all data\n",
    "* For multi-class classification, a “one versus all” approach is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "* unsupervised and supervised neighbors-based learning\n",
    "* classification & regression\n",
    "* Find n closest instances to the new point and predict label from these\n",
    "    * kNN -> k nearest, user defined\n",
    "    * Number ased on local density -> radius-based\n",
    "* Classification: non-generalizing ML model -> \"stores\" all training data & does not generlaize. A new point is then assigned based on the training data\n",
    "* Efficient when decision boundary is very irregular\n",
    "* **Unsupervised** NN: \n",
    "    * brute-force `sklearn.metrics.pairwise`: Distance of all pairs\n",
    "    * `KDTree`: if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant.\n",
    "    *`BallTree`: recursively divides data into nodes defined by a centroid of radius r\n",
    "* Best in small or medimum sized data\n",
    "* Nearest Neighbors Regression use-cases\n",
    "    * Impute missing values\n",
    "    * time series forecasting\n",
    "    * geographical/spatial data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "SVM vs kNN\n",
    "- KNN can handle <span style=\"color:#1df5b4;\">sparse data</span> well since it directly considers the distance between data points. SVM, particularly with linear kernels, may struggle in high-dimensional spaces due to the curse of dimensionality, leading to overfitting or suboptimal decision boundaries. \n",
    "- KNN's instance-based approach allows it to leverage <span style=\"color:#1df5b4;\">local</span> structures in sparse data effec KNN is particularly effective when local neighborhoods carry significant information about class membership. In image recognition, slight variations in pixel values may lead to different classes, and KNN can capture these local nuances. SVM, in such cases, might form more global boundaries that overlook fine-grained local patterns.\n",
    "- KNN can effectively <span style=\"color:#1df5b4;\">impute</span> missing values based on the nearest neighbors, allowing for flexible data handling. \n",
    "- SVM is effective in <span style=\"color:#1df5b4;\">high-dimensional</span> spaces due to its focus on maximizing the margin between classes. KNN can struggle with the curse of dimensionality, where all points seem equidistant,\n",
    "- SVM works on the principle of maximizing the margin and has well-defined theoretical foundations that help with <span style=\"color:#1df5b4;\">generalization</span>\n",
    "- SVM can be adapted to <span style=\"color:#1df5b4;\">multi-class</span> classification effectively through strategies like “one-vs-one” or “one-vs-all.” These adaptations can lead to good performance even in multi-class settings. KNN, although it can also handle multi-class problems, may become computationally expensive as the number of classes increases because it needs to consider multiple neighbors for each prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT: https://scikit-learn.org/stable/modules/gaussian_process.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn_certificate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
