{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning linear models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Unsupervised learning: extract from data X a structure that can be generalized\n",
    "\n",
    "#### Supervised learning - Linear models\n",
    "* Target is linear combination of feature: coef_, intercept_, .fit()\n",
    "* Linear regression: minimize the residual sum between observed and predicted target\n",
    "* Important that features are independent of each other\n",
    "* Evaluate with mean square error\n",
    "* Coefficients can be forced to be non-negative: positive parameter\n",
    "* Does not have a classifier with it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "reg.fit(X, y)\n",
    "print('coefficients', reg.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Ridge regression and classification](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)\n",
    "\n",
    "* Penalty **equal to the square of the magnitude of coefficients** on the coefficients of correlated predictors (**shrinks** them)\n",
    "* Tries to handle multicolinearity\n",
    "* good when you want to keep most featuers, but want to remove the effect of correlation\n",
    "* When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero.\n",
    "* comes with a classifier (`RidgeClassifier`) -> Least Squares Support Vector Machines with a linear kernel\n",
    "* Cross-validation: `RidgeCV` and `RidgeClassifierCV`: work in the same way as `GridSearchCV` except that it defaults to efficient Leave-One-Out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients  [0.44444444 0.44444444]\n",
      "intercept  0.11111111111111116\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit(X, y)\n",
    "print('coefficients ', reg.coef_)\n",
    "print('intercept ',  reg.intercept_)\n",
    "\n",
    "regCV = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below there are multiple alphas, for each the MSE is calculated and the optimal alpha is chosen based on the best negative mean squared error (default) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas  1e-06\n",
      "coefficients  [0.49999988 0.49999988]\n",
      "intercept  2.491287887096405e-07\n"
     ]
    }
   ],
   "source": [
    "regCV = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "regCV.fit(X, y)\n",
    "\n",
    "\n",
    "print('alpha ', regCV.alpha_)\n",
    "print('coefficients ', regCV.coef_)\n",
    "print('intercept ',  regCV.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Lasso regression](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "* Penalty **equal to the absolute value of the coefficients** on the coefficients\n",
    "* Can set coefficients to exactly zero\n",
    "* good when there are many featuers, and you want to reduce them\n",
    "* Cross-validation: there is \n",
    "    * `LassoCV`: \n",
    "        * coordinate descent -> optimizes one coefficient at a time \n",
    "        * simpler and straight forward implementation\n",
    "        * advised for data with many correlated features\n",
    "    * `LassoLarsCV`: \n",
    "        * optimizes all coefficients at the same time by moving the coefficients in the direction of the most correlated features and updates all coefficients simultaneously\n",
    "        * efficient in contexts where the number of features is significantly greater than the number of samples\n",
    "        * often faster than LassoCV, more relevant parameters of alpha\n",
    "\n",
    "\n",
    "Some notes on [being cautious with coefficients](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py):\n",
    "* coefficients vary significantly when changing the input dataset their robustness is not guaranteed (train/test) - two or more features could be correlated\n",
    "\n",
    "\n",
    "Other models:\n",
    "* `LassoLarsIC\" uses Akaike Information Criterion (AIC) and Mayes Information Criterion (BIC)\n",
    "* tend to break when the problem is badly conditioned (e.g. more features than samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn_certificate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
